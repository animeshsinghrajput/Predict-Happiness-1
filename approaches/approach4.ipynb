{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gradient Boosting as the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_neg = df[df['Is_Response'] == 'not happy']\n",
    "dummy_pos = df[df['Is_Response'] == 'not happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plt_count_freq(counts,title=None,_ylim=100000):\n",
    "    freq = list(set(counts.values()))\n",
    "    freq.sort()\n",
    "    freq.reverse()\n",
    "    plt.plot(freq)\n",
    "    plt.xlabel('Terms')\n",
    "    plt.ylabel('freq count')\n",
    "    plt.ylim(0,_ylim)\n",
    "    if(title!=None):\n",
    "        plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_words = ' '.join(dummy_neg['Description'])\n",
    "pos_words = ' '.join(dummy_pos['Description'])\n",
    "neg_words_list = word_tokenize(neg_words)\n",
    "pos_words_list = word_tokenize(pos_words)\n",
    "neg_counts = Counter(neg_words_list)\n",
    "pos_counts = Counter(pos_words_list)\n",
    "neg_most_common = neg_counts.most_common()\n",
    "pos_most_common = pos_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt_count_freq(counts,'Corpus')\n",
    "plt_count_freq(pos_counts,'Positive words',40000)\n",
    "plt_count_freq(neg_counts,'Negative Words',20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tf_high_stoplist(pos_counts,neg_counts,thresh=20000):\n",
    "    pos_words_thresh = []\n",
    "    stoplist = []\n",
    "    for word,freq in pos_counts.most_common():\n",
    "        if freq>=thresh:\n",
    "            pos_words_thresh.append(word)\n",
    "        else:\n",
    "            break\n",
    "    for word,freq in neg_counts.most_common():\n",
    "        if(freq<thresh):\n",
    "            break\n",
    "        elif(word in pos_words_thresh):\n",
    "            stoplist.append(word)\n",
    "    return stoplist    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_high_stoplist = create_tf_high_stoplist(pos_counts,neg_counts,7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(inputDF, tf1 = False,stops_bool = False,tf_high=False, updates=[],*tf_high_args):\n",
    "\n",
    "    if tf1:\n",
    "        file = open(\"TF1_cleaned_input.p\",'rb')\n",
    "        tf1_cleaned_data = pickle.load(file)\n",
    "        file.close()\n",
    "        stops = set()\n",
    "        if stops_bool:\n",
    "            stops = set(STOPWORDS)\n",
    "            if updates:\n",
    "                assert isinstance(updates,list)\n",
    "                stops.update(updates)\n",
    "            if tf_high:\n",
    "                stops.update(create_tf_high_stoplist(*tf_high_args))\n",
    "        else:\n",
    "            if tf_high:\n",
    "                stops.update(create_tf_high_stoplist(*tf_high_args))\n",
    "            if updates:\n",
    "                assert isinstance(updates,list)\n",
    "                stops.update(updates)\n",
    "        count_vec = CountVectorizer(analyzer = \"word\",stop_words= stops,max_features=500)\n",
    "        count_vec_ngrams = CountVectorizer(analyzer = \"word\", stop_words = stops , ngram_range = (1,2),max_features=500)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['sentiment']  = df['Is_Response'].apply(\n",
    "lambda x : 0 if x == 'not happy' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    26521\n",
       "0    12411\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputDF = df['Description']\n",
    "outputDF = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"TF1_cleaned_input.p\",'rb')\n",
    "tf1_cleaned_data = pickle.load(file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38932"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf1_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputDF = tf1_cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ' '.join(inputDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(inputDF,outputDF,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(STOPWORDS)\n",
    "stops.update(tf_high_stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(analyzer = \"word\",stop_words=stops)\n",
    "count_vec_ngrams = CountVectorizer(analyzer = \"word\", ngram_range = (1,2),stop_words=stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = count_vec.fit_transform(x_train)\n",
    "train_features_grams = count_vec_ngrams.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = count_vec.transform(x_test)\n",
    "test_features_grams = count_vec_ngrams.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingRegressor(n_estimators=100,max_depth=10,learning_rate=0.01,verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2158            4.14m\n",
      "         2           0.2143            3.79m\n",
      "         3           0.2129            3.61m\n",
      "         4           0.2115            3.52m\n",
      "         5           0.2101            3.43m\n",
      "         6           0.2088            3.39m\n",
      "         7           0.2075            3.35m\n",
      "         8           0.2062            3.31m\n",
      "         9           0.2049            3.28m\n",
      "        10           0.2037            3.23m\n",
      "        11           0.2025            3.20m\n",
      "        12           0.2013            3.17m\n",
      "        13           0.2001            3.16m\n",
      "        14           0.1989            3.18m\n",
      "        15           0.1978            3.17m\n",
      "        16           0.1966            3.13m\n",
      "        17           0.1955            3.08m\n",
      "        18           0.1944            3.04m\n",
      "        19           0.1933            2.99m\n",
      "        20           0.1923            2.96m\n",
      "        21           0.1912            2.92m\n",
      "        22           0.1902            2.88m\n",
      "        23           0.1892            2.84m\n",
      "        24           0.1882            2.80m\n",
      "        25           0.1873            2.77m\n",
      "        26           0.1863            2.73m\n",
      "        27           0.1853            2.72m\n",
      "        28           0.1844            2.69m\n",
      "        29           0.1835            2.65m\n",
      "        30           0.1826            2.61m\n",
      "        31           0.1817            2.57m\n",
      "        32           0.1808            2.53m\n",
      "        33           0.1800            2.49m\n",
      "        34           0.1791            2.47m\n",
      "        35           0.1783            2.43m\n",
      "        36           0.1774            2.40m\n",
      "        37           0.1766            2.36m\n",
      "        38           0.1758            2.32m\n",
      "        39           0.1750            2.29m\n",
      "        40           0.1742            2.26m\n",
      "        41           0.1734            2.24m\n",
      "        42           0.1727            2.20m\n",
      "        43           0.1719            2.16m\n",
      "        44           0.1712            2.12m\n",
      "        45           0.1704            2.08m\n",
      "        46           0.1697            2.04m\n",
      "        47           0.1690            2.00m\n",
      "        48           0.1683            1.96m\n",
      "        49           0.1676            1.92m\n",
      "        50           0.1669            1.88m\n",
      "        51           0.1663            1.84m\n",
      "        52           0.1656            1.80m\n",
      "        53           0.1649            1.77m\n",
      "        54           0.1642            1.74m\n",
      "        55           0.1636            1.70m\n",
      "        56           0.1629            1.66m\n",
      "        57           0.1623            1.62m\n",
      "        58           0.1616            1.58m\n",
      "        59           0.1610            1.54m\n",
      "        60           0.1604            1.50m\n",
      "        61           0.1598            1.47m\n",
      "        62           0.1592            1.43m\n",
      "        63           0.1586            1.39m\n",
      "        64           0.1580            1.35m\n",
      "        65           0.1574            1.31m\n",
      "        66           0.1568            1.28m\n",
      "        67           0.1563            1.25m\n",
      "        68           0.1557            1.21m\n",
      "        69           0.1552            1.17m\n",
      "        70           0.1546            1.13m\n",
      "        71           0.1541            1.09m\n",
      "        72           0.1535            1.05m\n",
      "        73           0.1530            1.02m\n",
      "        74           0.1525           58.54s\n",
      "        75           0.1520           56.24s\n",
      "        76           0.1514           53.95s\n",
      "        77           0.1510           51.50s\n",
      "        78           0.1505           49.15s\n",
      "        79           0.1500           46.72s\n",
      "        80           0.1495           44.60s\n",
      "        81           0.1490           42.29s\n",
      "        82           0.1485           40.12s\n",
      "        83           0.1480           37.88s\n",
      "        84           0.1475           35.61s\n",
      "        85           0.1471           33.25s\n",
      "        86           0.1466           31.00s\n",
      "        87           0.1461           28.75s\n",
      "        88           0.1456           26.55s\n",
      "        89           0.1452           24.25s\n",
      "        90           0.1447           22.02s\n",
      "        91           0.1443           19.80s\n",
      "        92           0.1438           17.59s\n",
      "        93           0.1434           15.37s\n",
      "        94           0.1429           13.18s\n",
      "        95           0.1425           10.99s\n",
      "        96           0.1420            8.81s\n",
      "        97           0.1416            6.59s\n",
      "        98           0.1412            4.40s\n",
      "        99           0.1408            2.19s\n",
      "       100           0.1404            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='ls', max_depth=10,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=2,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25341955559682328"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_features,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         0.00000000e+00,   0.00000000e+00,   3.78283105e-06])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting is performing really bad, so ignoring this approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf_grams = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81064420014384053"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_features,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grams.fit(train_features_grams,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79605465940614406"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grams.score(test_features_grams,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756087537244\n",
      "0.747970820919\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf_grams = DecisionTreeClassifier()\n",
    "clf.fit(train_features,y_train)\n",
    "clf_grams.fit(train_features_grams,y_train)\n",
    "print(clf_grams.score(test_features_grams,y_test))\n",
    "print(clf.score(test_features,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.870440768519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86098839001335659"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf_grams = MultinomialNB()\n",
    "clf_grams.fit(train_features_grams,y_train)\n",
    "clf.fit(train_features,y_train)\n",
    "print(clf_grams.score(test_features_grams,y_test))\n",
    "clf.score(test_features,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TF = TfidfVectorizer()\n",
    "train_features = TF.fit_transform(x_train)\n",
    "test_features = TF.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74190896948525631"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_features,y_train)\n",
    "clf.score(test_features,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82862426795438204"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(train_features,y_train)\n",
    "clf.score(test_features,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_words = word_tokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = Counter(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_high_stoplist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### both have set of words whose frequencies higher than 20,000, so making a stoplist of those common keys, terms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = count_vec.fit_transform(inputDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38932, 24081)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "testing_df = testDF['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = count_vec.transform(testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_features,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29404,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDF['Is_Response'] = prediction\n",
    "testDF['Is_Response'] = testDF['Is_Response'].apply(lambda x:'not_happy' if x==0 else 'happy') \n",
    "submissionDF = testDF[['User_ID','Is_Response']]\n",
    "submissionDF.to_csv('submissions/submission_4.csv',columns=['User_ID','Is_Response'],index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80.878 % accuracy with RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
